#  Q7 - Evaluate on the test set

After training and validating several models, it is now time to assess their generalizability on independent test sets. We will consider: the logistic regression with distance (*LogReg: dist*), the logistic regression with angle (*LogReg: angle*), the logistic regression with distance and angle (*LogReg: dist & angle*), the reduced XGBoost model (*XGBoost reduced*), and the LightGBM model (*LGBM*). Each trained model will be evaluated on the games from the regular season of 2019-2020, and the games from the playoffs of 2019-2020.

## Regular games 2019-2020
First observation, the *LogReg: dist*, and *LogReg: dist & angle* almost completly overlay in each plot. Since *LogReg: angle* makes almost random prediction (AUC=0.51), it suggests that *LogReg: dist & angle* almost exclusively relies on the distance information, which also appeared as the most important feature in XGBoost models.

Additionally, the XGBoost and LGBM also display almost identical metrics, which is consistent with the fact that they both are gradient boosting machines. However, the calibration curve suggests a better calibration for the *XGBoost reduced*, close to the one observed during its training. This suggests that the model generalized well to this test set. This could be the result of the reduced set of features, limiting the influencing of "irrelevant" or "noisy" features.

## Playsoffs 2019-2020
The main distinction between the Regular season and Playoffs test set is the performance of the XGBoost model. Indeed, *XGBoost reduced*'s performed worst on all indicators, even falling below the *LogReg* models in terms of ROC AUC. A first hypothesis is that the advanced features are too noisy to be helpful, especially in a small test set where the variance is increased. An alternative hypothesis would be that playoffs games are qualitatively different from regular game, such that the learning from the *XGBoost reduced* did not generalized.

An interesting source of insight is the sustained performance of the *LGBM* model. Since XGBoost and LGBM are both members of the same family, what lead to their disparity in performance? It would be surprising that it truly is the feature set that distinguishes because only the least relevant features were dropped. This only raises more questions to investigate, in particular, are there differences in robusteness between the algorithms, or regularization within their hyperparameters.