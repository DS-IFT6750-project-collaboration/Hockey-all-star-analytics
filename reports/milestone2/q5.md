# Q5 - Advanced Models

1. First, a baseline was established by training an XGBoost classifier with only the features *distance* from the net and *angle* from the net (*XGBoost base*). This XGBoost baseline model exhibited a ROC area under the curve of 0.72, which is an improvement over the logistic regression baseline. The calibration curve suggests the model has difficult formulating appropriate predictions, by under evaluating between 0.2 and ~0.7, and then overshooting.

Similarly to the logistic regression models trained, a 80:20 split was used for training and validation. The classifier was trained with default hyperparameter values of 100 estimators, a max tree depth of 4, and a learning rate of 1. No hyperparameter tuning was conducted. (Link to model: )

2. Then, another XGBoost classifier was trained using the advanced features developed, and was tuned for best performance (*XGBoost tuned*). This model has an ROC AUC of 0.76, which is an improvement over the XGBoost base. More importantly, the XGBoost tuned seem way better calibrated, with some overshoot after ~0.7.

XGBoost models have a large amount of hyperparameters to modify. For the purpose of this task, the search space was defined over the following features: the max tree depth (*max_depth*), the learning rate, the number of estimators, the L1 regularization, the L2 regularization, and the minimum weight of a node before splitting (*min_child_weight*). They were selected because they play an important role in the model's ability to learn, and to generalize.

The optimization was completed with the library *optuna*, which allows to define an *objective* that needs to be optimize by iterating through hyperparameters over several *trials*. The objective was to minimize the average of "best logloss" of a model with a set of hyperparameters trained 5 times over different cross-validation splits. Different hyperparameter configurations were tested over 20 trials. (Link to model: )

3. Finally, the feature importance of the XGBoost tuned model was inspected with the goal of reducing the feature set, and training another classifier (*XGBoost reduced*). The overall performance is almost identical to the *XGBoost tuned*, with the same ROC AUC of 0.76. The calibration curve suggests the *XGBoost reduced* is better calibrated with less overshoot, but drifts off slightly into undershooting.

Using the XGBoost package, the feature importance was defined as the *gain* by splitting on a feature. Based on the plot, the following features were removed: *period_idx*, *PERIOD_START* (event_type category), *PENALTY* (event_type category), *x_coord*, and *y_coord*. The coordinates were removed because they seemed redundant with their normalized counterparts. Also, the events *BLOCKED_SHOT*, *MISSED_SHOT*, and *SHOT* were combined since they all represented "shot" events, but were not too important individually. Also, it should be noted that the feature *empty_net* was removed from the *XGBoost tuned* and *XGBoost reduced* because it was leaking information about the labels (see figure).

Once the feature subset was determined, the *XGBoost reduced* was instanciated with the best parameters of the *XGBoost tuned* model, then trained on a single train-validation split. (Link to model: )



